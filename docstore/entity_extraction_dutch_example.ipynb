{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ingel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ingel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ingel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ingel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\ingel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('At', 'IN'),\n",
       " ('eight', 'CD'),\n",
       " (\"o'clock\", 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Thursday', 'NNP'),\n",
       " ('morning', 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    728\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0min_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    729\u001b[0m             \u001b[0m_canvas_frame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroy_widget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 730\u001b[1;33m             subprocess.call([find_binary('gs', binary_names=['gswin32c.exe', 'gswin64c.exe'], env_vars=['PATH'], verbose=False)] +\n\u001b[0m\u001b[0;32m    731\u001b[0m                             \u001b[1;34m'-q -dEPSCrop -sDEVICE=png16m -r90 -dTextAlphaBits=4 -dGraphicsAlphaBits=4 -dSAFER -dBATCH -dNOPAUSE -sOutputFile={0:} {1:}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    732\u001b[0m                             .format(out_path, in_path).split())\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    602\u001b[0m                 binary_names=None, url=None, verbose=False):\n\u001b[0;32m    603\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[1;32m--> 604\u001b[1;33m                                  binary_names, url, verbose))\n\u001b[0m\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    596\u001b[0m     \"\"\"\n\u001b[0;32m    597\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[1;32m--> 598\u001b[1;33m                      url, verbose):\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    567\u001b[0m                         (filename, url))\n\u001b[0;32m    568\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n==========================================================================="
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [('At', 'IN'), ('eight', 'CD'), (\"o'clock\", 'NN'), ('on', 'IN'), ('Thursday', 'NNP'), ('morning', 'NN'), Tree('PERSON', [('Arthur', 'NNP')]), ('did', 'VBD'), (\"n't\", 'RB'), ('feel', 'VB'), ('very', 'RB'), ('good', 'JJ'), ('.', '.')])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour M. Adam, comment allez-vous?', \"J'espère que tout va bien.\", \"Aujourd'hui est un bon jour.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "mytext = \"Bonjour M. Adam, comment allez-vous? J'espère que tout va bien. Aujourd'hui est un bon jour.\"\n",
    " \n",
    "print(sent_tokenize(mytext,\"french\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Zijn naam is Sam Vanloocke.', 'Hij woont in Gent met zijn verloofde, Nele Victor.', 'Nele en Sam wonen in een appartement aan de Lieve.', 'Sam is een werknemer van Custodix']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "mytext = \"Zijn naam is Sam Vanloocke. Hij woont in Gent met zijn verloofde, Nele Victor. Nele en Sam wonen in een appartement aan de Lieve. Sam is een werknemer van Custodix\"\n",
    " \n",
    "print(sent_tokenize(mytext,\"dutch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltokens = nltk.word_tokenize(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Zijn',\n",
       " 'naam',\n",
       " 'is',\n",
       " 'Sam',\n",
       " 'Vanloocke',\n",
       " '.',\n",
       " 'Hij',\n",
       " 'woont',\n",
       " 'in',\n",
       " 'Gent',\n",
       " 'met',\n",
       " 'zijn',\n",
       " 'verloofde',\n",
       " ',',\n",
       " 'Nele',\n",
       " 'Victor',\n",
       " '.',\n",
       " 'Nele',\n",
       " 'en',\n",
       " 'Sam',\n",
       " 'wonen',\n",
       " 'in',\n",
       " 'een',\n",
       " 'appartement',\n",
       " 'aan',\n",
       " 'de',\n",
       " 'Lieve',\n",
       " '.',\n",
       " 'Sam',\n",
       " 'is',\n",
       " 'een',\n",
       " 'werknemer',\n",
       " 'van',\n",
       " 'Custodix']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Zijn', 'NNP'), ('naam', 'NN'), ('is', 'VBZ'), ('Sam', 'NNP'), ('Vanloocke', 'NNP'), ('.', '.'), ('Hij', 'NNP'), ('woont', 'NN'), ('in', 'IN'), ('Gent', 'NNP'), ('met', 'VBD'), ('zijn', 'NNP'), ('verloofde', 'NN'), (',', ','), ('Nele', 'NNP'), ('Victor', 'NNP'), ('.', '.'), ('Nele', 'NNP'), ('en', 'IN'), ('Sam', 'NNP'), ('wonen', 'NN'), ('in', 'IN'), ('een', 'JJ'), ('appartement', 'JJ'), ('aan', 'NN'), ('de', 'IN'), ('Lieve', 'NNP'), ('.', '.'), ('Sam', 'NNP'), ('is', 'VBZ'), ('een', 'JJ'), ('werknemer', 'JJ'), ('van', 'NN'), ('Custodix', 'NNP')]\n",
      "(S\n",
      "  (GPE Zijn/NNP)\n",
      "  naam/NN\n",
      "  is/VBZ\n",
      "  (PERSON Sam/NNP Vanloocke/NNP)\n",
      "  ./.\n",
      "  Hij/NNP\n",
      "  woont/NN\n",
      "  in/IN\n",
      "  (GPE Gent/NNP)\n",
      "  met/VBD\n",
      "  zijn/NNP\n",
      "  verloofde/NN\n",
      "  ,/,\n",
      "  (PERSON Nele/NNP Victor/NNP)\n",
      "  ./.\n",
      "  (PERSON Nele/NNP)\n",
      "  en/IN\n",
      "  (PERSON Sam/NNP)\n",
      "  wonen/NN\n",
      "  in/IN\n",
      "  een/JJ\n",
      "  appartement/JJ\n",
      "  aan/NN\n",
      "  de/IN\n",
      "  (PERSON Lieve/NNP)\n",
      "  ./.\n",
      "  (PERSON Sam/NNP)\n",
      "  is/VBZ\n",
      "  een/JJ\n",
      "  werknemer/JJ\n",
      "  van/NN\n",
      "  (PERSON Custodix/NNP))\n"
     ]
    }
   ],
   "source": [
    "nltagged = nltk.pos_tag(nltokens)\n",
    "nlentities = nltk.chunk.ne_chunk(nltagged)\n",
    "print(nltagged)\n",
    "print(nlentities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlentities.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\ingel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2002.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import conll2002\n",
    "nltk.download('conll2002')\n",
    "vnv = \"\"\"\n",
    " (\n",
    " is/V|    # 3rd sing present and\n",
    " was/V|   # past forms of the verb zijn ('be')\n",
    " werd/V|  # and also present\n",
    " wordt/V  # past of worden ('become)\n",
    " )\n",
    " .*       # followed by anything\n",
    " van/Prep # followed by van ('of')\n",
    " \"\"\"\n",
    "VAN = re.compile(vnv, re.VERBOSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PER Cornet/V d'Elzius/N)\n",
      "  is/V\n",
      "  op/Prep\n",
      "  dit/Pron\n",
      "  ogenblik/N\n",
      "  kabinetsadviseur/N\n",
      "  van/Prep\n",
      "  staatssecretaris/N\n",
      "  voor/Prep\n",
      "  (ORG Buitenlandse/N Handel/N)\n",
      "  (PER Pierre/N Chevalier/N)\n",
      "  (/Punc\n",
      "  (ORG VLD/N)\n",
      "  )/Punc\n",
      "  ./Punc)\n",
      "VAN(\"cornet_d'elzius\", 'buitenlandse_handel')\n",
      "(S\n",
      "  (PER Johan/N Rottiers/N)\n",
      "  is/V\n",
      "  informaticacoördinator/N\n",
      "  van/Prep\n",
      "  het/Art\n",
      "  (ORG Kardinaal/N Van/N Roey/N Instituut/N)\n",
      "  in/Prep\n",
      "  (LOC Vorselaar/N)\n",
      "  ./Punc)\n",
      "VAN('johan_rottiers', 'kardinaal_van_roey_instituut')\n",
      "(S\n",
      "  Door/Prep\n",
      "  rugproblemen/N\n",
      "  van/Prep\n",
      "  zangeres/N\n",
      "  (PER Annie/N Lennox/N)\n",
      "  wordt/V\n",
      "  het/Art\n",
      "  concert/N\n",
      "  van/Prep\n",
      "  (ORG Eurythmics/N)\n",
      "  vandaag/Adv\n",
      "  in/Prep\n",
      "  (MISC Werchter/N)\n",
      "  geannuleerd/V\n",
      "  ./Punc)\n",
      "VAN('annie_lennox', 'eurythmics')\n"
     ]
    }
   ],
   "source": [
    "for doc in conll2002.chunked_sents('ned.train'):\n",
    "    for r in nltk.sem.extract_rels('PER', 'ORG', doc, corpus='conll2002', pattern=VAN):\n",
    "        print(doc)\n",
    "        print(nltk.sem.clause(r, relsym=\"VAN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n (\\n is/V|    # 3rd sing present and\\n was/V|   # past forms of the verb zijn ('be')\\n werd/V|  # and also present\\n wordt/V  # past of worden ('become)\\n )\\n .*       # followed by anything\\n van/Prep # followed by van ('of')\\n \""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for doc in conll2002.chunked_sents('ned.train'):\n",
    "    for r in nltk.sem.extract_rels('PER', 'ORG', doc, corpus='conll2002', pattern=VAN):\n",
    "        print(doc)\n",
    "        print(nltk.sem.clause(r, relsym=\"VAN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in nltk.sem.extract_rels('PER', 'ORG', nlentities, corpus='conll2002', pattern=VAN):\n",
    "    print(nltk.sem.clause(r, relsym=\"VAN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['esp.testa', 'esp.testb', 'esp.train', 'ned.testa', 'ned.testb', 'ned.train']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2002.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202644"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl = conll2002.words('ned.train')\n",
    ">>> len(nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"Vandenbussche van Floralux is niet vermist\"\n",
    "tok = nltk.word_tokenize(txt, language='dutch')\n",
    "tag = nltk.pos_tag(tok, lang='nld')\n",
    "ent = nltk.chunk.ne_chunk(tag)\n",
    "for r in nltk.sem.extract_rels('PER', 'ORG', ent, corpus='conll2002', pattern=VAN):\n",
    "    print(nltk.sem.clause(r, relsym=\"VAN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Vandenbussche/NNP)\n",
      "  van/NN\n",
      "  (PERSON Floralux/NNP)\n",
      "  is/VBZ\n",
      "  niet/JJ\n",
      "  vermist/NN)\n"
     ]
    }
   ],
   "source": [
    "print(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
